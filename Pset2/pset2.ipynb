{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 2, by M. Kaledin\n",
    "\n",
    "# Problem 1 (LU decomposition)\n",
    "## 30 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LU for band matrices (5 pts)\n",
    "\n",
    "The complexity to find an LU decomposition of a dense $n\\times n$ matrix is $\\mathcal{O}(n^3)$.\n",
    "Significant reduction in complexity can be achieved if the matrix has a certain structure, e.g. it is sparse. \n",
    "In the following task we consider an important example of $LU$ for a special type of sparse matrices –– tridiagonal matrices.\n",
    "\n",
    "- Find the number of operations to compute an $LU$ decomposition of a tridiagonal matrix taking into account only non-zero elements. How many nonzero elements are in factors $L$, $U$ and where are they located? Conclude what is the complexity to solve a linear system with tridiagonal matrix in terms of $n$. \n",
    "\n",
    "#### Solution\n",
    "\n",
    "I will use the similar approach as for usual LU-decomposition. Suppose that matrix A has entries (near the diagonal) $d_k,b_k,c_k,~k=1,..,n$, $d_1=c_n=0$ do not appear, and LU-decomposition exists. Let $l_{ii}=1$ for all $i=1,..,n$ for normaliztion. Start with $b_{11}=l_{11}u_{11}=u_{11}$, so $u_{11}=b_1$. Then we go down and write $l_{i1}u_11=a_{i1}$, however $a_{i1}=0$ for $i>2$ and $a_{21}=d_2$. Thus $l_{21}=d_2/u_{11}$ and $l_{i1}=0$ for $i>2$. Now go right and write down $l_{11}u_1i=a_{1i}$, so $u_{12}=c_1$ and $u_{1i}=0$ for $i>2$.\n",
    "\n",
    "Now use the induction method, suppose for $k-1<n-1$ the following holds (these statements can be easily checked for $k-1=2$, so we have obtained the induction base): \n",
    "\n",
    "1. $u_{k-1,k-1}=b_{k-1}-u_{k-2,k-1}l_{k-2,k-1}$,   \n",
    "2. $l_{k,k-1}=d_{k-1}/u_{k-1,k-1}$ and $l_{t,k-1}=0$ for $t>k$.\n",
    "3. $u_{k-1,k}=c_k$ and $u_{k-1,t}=0$ for $t>k$.\n",
    "\n",
    "Consider case $k$, so we have already obtained all $u_{ij}$ for $i<k,j\\geq k$\n",
    " and $l_{ij}$ for $i\\geq k,j<k$. Directly we can derive that\n",
    " \n",
    " $$a_{kk}=b_k=u_{k-1,k}l_{k,k-1}+u_{kk}l_{kk},$$\n",
    " \n",
    " hence $u_{kk}=b_k-u_{k-1,k}l_{k,k-1}$. Go down and write $a_{k+1,k}=d_{k+1}$ and zero otherwise. So\n",
    "\n",
    " $$\n",
    "     d_{k+1}=l_{k+1,k}u_{k,k} \\Rightarrow l_{k+1,k}=\\frac{d_{k+1}}{u_{k,k}}.\n",
    " $$\n",
    " One can note that $l_{i,k}=0$ for $i>k+1$ due to $u_{k,k}\\neq 0$ and $a_{ik}=0$ for $i>k+1$. Finally,\n",
    " \n",
    " $$\n",
    "     c_k=l_{k-1,k-1}u_{k,k+1}=u_{k,k+1},\n",
    " $$\n",
    " moreover, $u_{k,i}=0$ for $i>k+1$ because $a_{k,i}=0$ for $i>k+1$. By induction the proposed formulas hold for any $k<n$. For case $k=n$  $b_n=l_{n,n-1}u_{n-1,n}+u_{nn}l_{nn}$ implies $u_{nn}=b_n - l_{n,n-1}u_{n-1,n}$. This is the last needed coefficient.\n",
    " \n",
    " With such procedure we have done $1+n-2$ divisions, $n-1$ multiplications and $n-1$ additions, the result complexity is $3(n-1)$ operations which is $O(n)$.\n",
    " \n",
    "### 2. Completing the proof of existence of LU (10 pts)\n",
    "\n",
    "Some details in lecture proofs about $LU$ were omitted. Let us complete them.\n",
    "- Prove that if $LU$ decomposition exists, then matrix is strictly regular.\n",
    "\n",
    "Assume $l_ii=1$ (w.l.o.g.). Suppose the matrix is not strictly regular, then $\\exists k$ such that the $k$-th principal minor $\\text{det}(A_k)=0$. This minor can be expanded using LU $A_k = L_kU_k$ where $L_k,U_k$ are $L,U$ with first $k$ rows. By the property of determinant $\\text{det}(A_k)=\\text{det}(L_k)\\text{det}(U_k)=\\text{det}(U_k)=u_11\\cdot...\\cdot u_kk$. Since $\\text{det}(A_k)=0$ then there exists $u_{tt}=0$ for some $t \\leq k$. We can write $a_{tt}=\\sum_{x=1}^{t-1}l_{tx}u_{xt}+u_tt=\\sum_{x=1}^{t-1}l_{tx}u_{xt}$ and $a_{\\xi,t}=\\sum_{x=1}^{t-1}l_{\\xi,x}u_{x,t}+l_{\\xi t}u_{tt}=\\sum_{x=1}^{t-1}l_{\\xi,x}u_{x,t}$.  \n",
    "\n",
    "- Prove that if $A$ is strictly regular, then $A_1 = D - \\frac 1a b c^T$ (see lectures for notations) is also strictly regular.\n",
    "\n",
    "### 3. Stability of LU (10 pts)\n",
    "\n",
    "Let\n",
    "$A = \\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}.$ \n",
    "* Find analytically an $LU$ decomposition of the matrix $A$.\n",
    "\n",
    "Subtract 1 from 2 (considering $a\\neq 0$):\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-1/a & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix},\n",
    "$\n",
    "\n",
    "corresponding elementary matrix is \n",
    "\n",
    "$E_1=\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "-1/a & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}.$\n",
    "\n",
    "Substract 2 from 3 (considering $1-1/a\\neq 0$):\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-1/a & 1 \\\\\n",
    "0 & 0 & 1/(1-a)\n",
    "\\end{pmatrix}=U,\n",
    "$\n",
    "\n",
    "corresponding elementary matrix is \n",
    "\n",
    "$E_2=\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & -1/(1-1/a) & 1\n",
    "\\end{pmatrix}.$\n",
    "\n",
    "So $A= (E_2E_1)^{-1}U=E_1^{-1}E_2^{-1}U$. Matrix $L=E_1^{-1}E_2^{-1}$ can be easily computed because $E_1^{-1},E_2^{-1}$ are elemtary matrices corresponding to subtract1From2 and subtract2From3. Thus\n",
    "$$\n",
    "L=E_1^{-1}E_2^{-1}=\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "1/a & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 1/(1-1/a) & 1\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "1/a & 1 & 0 \\\\\n",
    "0 & 1/(1-1/a) & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* For what values of $a$ does the LU decomposition of $A$ exist?\n",
    "\n",
    "To ensure that LU decomposition exists we need to show that $A$ is strictly regular. So all principal minors\n",
    "\n",
    "1. $a\\neq 0$,\n",
    "2. $a-1 \\neq 0$,\n",
    "3. $det(A)=-1 \\neq 0$ already.\n",
    "\n",
    "Hence for $a \\neq 0, 1$ such decomposition exists.\n",
    "\n",
    "* Explain, why can the LU decomposition fail to approximate factors $L$ and $U$ for $|a|\\ll 1$ in computer arithmetic?\n",
    "How can this problem be solved?\n",
    "\n",
    "In LU-decomposition we need to compute $l_{21}=a_{21}/u_{11}=a_{21}/a_{11}=1/a$. When $a=10^{-k}$ is small we multiply by very large number what could lead to a loss of significance. \n",
    "\n",
    "$$\n",
    "u_{22}=a_{22}-l_{21}u_{12}=a_{22}-l_{21}a_{21}/a_{11}=1-(1/a)/ a\n",
    "a_{22}=1=l_{21}u_{12}+u_{22}=1/a*\\left( (1/a)/a \\right)+ (1-(1/a)/ a) \\approx 10^{3k}-10^{2k} >> 1.\n",
    "$$\n",
    "\n",
    "One possible solution is row (column, or both) pivoting. On each iteration of algorithm (computing k-th column and k-th row of decomposition) we permute rows and columns matrix in such a way that the maximal (in absolute value) element goes on place of $a_{kk}$. Instead of usual LU decomposition we search for\n",
    "\n",
    "$$A=PLUP', $$\n",
    "\n",
    "where $P,P'$ are permutation matrices. This approach is numerically more stable than usual LU.\n",
    "\n",
    "### 4. Block LU (5 pts)\n",
    "\n",
    "Let $A = \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix}$ be a block matrix. The goal is to solve the linear system\n",
    "$$\n",
    "     \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "* Using block elimination find matrix $S$ and right-hand side $f_2$ so that $u_2$ can be found from $S u_2 = f_2$. Note that the matrix $S$ is called <font color='red'> Schur complement </font> of the block $A_{11}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (QR decomposition) \n",
    "\n",
    "## 20 pts\n",
    "\n",
    "### 1. Standard Gram-Schmidt algorithm (10 pts)\n",
    "Our goal now is to orthogonalize a system of linearly independent vectors $v_1,\\dots,v_n$.\n",
    "The standard algorithm for the task is the Gram-Schmidt algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "u_1 &= v_1, \\\\\n",
    "u_2 &= v_2 - \\frac{(v_2, u_1)}{(u_1, u_1)} u_1, \\\\\n",
    "\\dots \\\\\n",
    "u_n &= v_n - \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 - \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 - \\dots - \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Now $u_1, \\dots, u_n$ are orthogonal vectors in exact arithmetics. Then to get orthonormal system you should divide each of the vectors by its norm: $u_i := u_i/\\|u_i\\|$.\n",
    "The Gram-Schidt process can be viewed as a QR decomposition. Let us show that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write out what is $Q$ and $R$ obtained in the process described. \n",
    "\n",
    "### Solution\n",
    "\n",
    "Let $v_i$ be $n$-dimensional linearily independent vectors. The above expressions could be rewritten:\n",
    "\n",
    "$$\\begin{split}\n",
    "&v_1=u_1,\\\\\n",
    "&v_2=u_2+\\frac{(v_2, u_1)}{(u_1, u_1)} u_1,\\\\\n",
    "&v_3=u_3+\\frac{(v_3, u_1)}{(u_1, u_1)} u_1+\\frac{(v_3, u_2)}{(u_2, u_2)} u_2,\\\\\n",
    "...\n",
    "\\end{split}\n",
    "$$\n",
    "This can be rewritten as the matrix equation:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "v_1 & v_2 & ... & v_m\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "u_1 & u_2 & ... & u_m\n",
    "\\end{pmatrix}R=QR,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "R_{ij}=\\begin{cases}\n",
    "0, \\text{if $i>j$},\\\\\n",
    "1, \\text{if $i=j$},\\\\\n",
    "(a_j,u_i), \\text{if $i<j$}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To get exact $QR$ we need to normalize all $u_i$ and set $R_{ii}=\\Vert u_i\\Vert$.\n",
    "\n",
    "* Implement the described Gram-Schmidt algorithm as a function ```gram_schmidt(A)```, which outputs ```Q,R``` and check it on a random $100\\times 100$ matrix $B.$ Print out the error. \n",
    "\n",
    "**Note:** To check orthogonality calculate the matrix of scalar products $G_{ij} = (u_i, u_j)$ (called <font color='red'> Gram matrix </font> of set of vectors $u_1,\\dots, u_n$) which should be equal to the identity matrix $I.$ Error $\\|G - I\\|_2$ will show you how far is the system $u_i$ from orthonormal.\n",
    "\n",
    "\n",
    "* Create a Hilbert matrix $A$ of size $100\\times 100$ without using loops.\n",
    "Othogonalize its columns by the described Gram-Schmidt algorithm. Is the Gram matrix close to the identity matrix in this case? Why?\n",
    "\n",
    "\n",
    "The observed loss of orthogonality is a problem of this particular algorithm. To avoid it [modified Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process#Numerical_stability), QR via Householder reflections or Givens rotations can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT : rectangular matrix A  n x m\n",
    "# OUTPUT: matrices Q - orthogonal and R - upper triangular such that A=QR\n",
    "\n",
    "def gram_schmidt(A): # 5 pts\n",
    "    #considering m >= n\n",
    "    \n",
    "    Q=np.zeros(A.shape)\n",
    "    R=np.eye(A.shape[1])\n",
    "    Q[:,0]=A[:,0]\n",
    "    R[0,1:]=Q[:,0].reshape(1,Q.shape[0]).dot(A[:,1:]/np.dot(Q[:,0],Q[:,0]))\n",
    "    R[0,0]=np.linalg.norm(Q[:,0],ord=2)\n",
    "    for k in np.arange(1,R.shape[0]-1):\n",
    "        Q[:,k]=A[:,k]-Q[:,:k].dot(R[0:k,k])\n",
    "        R[k,(k+1):]=Q[:,k].reshape(1,Q.shape[0]).dot(A[:,(k+1):]/np.dot(Q[:,k],Q[:,k]))\n",
    "        R[k,k]=np.linalg.norm(Q[:,k],ord=2)\n",
    "    Q[:,R.shape[0]-1]=A[:,R.shape[0]-1]-Q[:,:(R.shape[0]-1)].dot(R[0:(R.shape[0]-1),(R.shape[0]-1)])\n",
    "    Q=Q/np.linalg.norm(Q,ord=2,axis=0)\n",
    "    # enter your code here\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.98606229465e-12\n"
     ]
    }
   ],
   "source": [
    "B = np.random.uniform(0,10,size=(100,100))\n",
    "(Q_B,R_B)=gram_schmidt(B)\n",
    "print(np.linalg.norm(Q_B.T.dot(Q_B)-np.eye(100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0333629998\n"
     ]
    }
   ],
   "source": [
    "(xx,yy)=np.meshgrid(np.arange(1,101),np.arange(1,101))\n",
    "\n",
    "A = 1/(xx+yy-1) \n",
    "\n",
    "(Q_A,R_A)=gram_schmidt(A)\n",
    "print(np.linalg.norm(Q_A.T.dot(Q_A)-np.eye(100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the norms of columns of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f077b87c18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAGHCAYAAACu4BXOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f07517e908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(np.linalg.norm(A,ord=2,axis=0))\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(r\"$\\Vert v_k \\Vert$\")\n",
    "ax.set_title(\"Norms of columns of A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter the loss of significance when performing orthogonaliztion. The vector $u_1$ has the order of about $1$, then $u_2$ is also something similar. However, norms of vectors $u_2$ and further are of order at least $10^{-1}$. So the expression $(v_n,u_k)/(u_k,u_k)\\cdot u_k$ is of order approximately $10^{O(n)}$. When performing large iterations there is a loss a significance: latter terms dominate the first ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Householder QR (10 pts)\n",
    "\n",
    "* Implement Householder QR decomposition as a function ```householder_qr(A)``` which outputs ```Q,R```. Apply it to the matrix $B$ created above. Print out the error.\n",
    "\n",
    "\n",
    "* Apply it to the Hilbert matrix $A$ created in the first part of the problem and print out the error. Consider how stable is Householder compared to Gram-Schmidt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT : rectangular matrix A\n",
    "# OUTPUT: matrices Q - orthogonal and R - upper triangular such that A=QR\n",
    "def householder_qr(A): # 7 pts\n",
    "    # enter your code here\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 (Low-rank decompositions)\n",
    "\n",
    "## 45 pts\n",
    "\n",
    "## 1. Theoretical tasks (15 pts)\n",
    "\n",
    "* Prove that for any Hermitian matrix, singular values equal to absolute value of eigenvalues. Does this hold for a general matrix? Prove or provide a counterexample.\n",
    "\n",
    "\n",
    "Let $A$ be a square Hermitian matrix, i.e. $A^*=A$. By the famous theorem from linear algebra, the eigenvalues are real, and, moreover, there is an orthogonal basis of eigenvectors, so we could decompose $A=T\\Lambda T^{-1}$, where $T$ is a matrix of eigenvectors, $\\Lambda = \\text{diag}(\\text{Spec}A)$. Since the eigenvalues are real, we could introduce a diagonal matrix of signs $\\text{sgn}(\\Lambda)$ and absolute values $\\vert \\Lambda \\vert$. So $A=T \\vert\\Lambda\\vert \\text{sgn}(\\Lambda) T^{-1}=T \\vert\\Lambda\\vert \\tilde{T}$. The basis is orthogonal, hence $T$ and $\\tilde{T}$ are unitary matrices, $\\vert\\Lambda\\vert$ is a non-negative diagonal matrix, so $T \\vert\\Lambda\\vert \\tilde{T}$ is, by definition, an SVD-decomposition of $A$. \n",
    "\n",
    "* Find analytically a skeleton decomposition of the matrix of size $n\\times m$ with elements $a_{ij} = \\sin i + \\sin j$.\n",
    "\n",
    "\n",
    "* Let $A\\in\\mathbb{C}^{n\\times m}$ be of rank $r$ and let $A = U\\Sigma V^*$ be its SVD. Prove that $\\mathrm{im}(A^*) = \\mathrm{span}\\{v_1,\\dots, v_r\\}$, where $V = [v_1, \\dots, v_n]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommender system using SVD (30 pts)\n",
    "\n",
    "In this task you are asked to build a simple movie recommender system based on *collaborative filtering* approach and SVD.\n",
    "Collaborative filtering implies that you build recommendations based on the feedback of other users given in a matrix $\\mathbf{M}$ of users vs. movies. \n",
    "If a user $i$ watched a movie $j$ and rated it, say, as $3$ out of $5$, then the value $3$ is the corresponding matrix entry, i.e. $\\mathbf{M}_{i,j}=3$.\n",
    "If a user did not watch a movie, then we put $0$ as a matrix element, i.e. $\\mathbf{M}=0$. \n",
    "Hence, the matrix is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Building the core of recommender (15 pts)\n",
    "\n",
    "Build representation of users and movies in the latent factors space with the help of SVD.\n",
    "\n",
    "* We test the SVD model on a [Movielens 10M](https://grouplens.org/datasets/movielens/) dataset. Download the dataset using python functions provided in the following [Jupyter notebook](movielens10m.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into memory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import movielens10m #in order not to include the large code here,\n",
    "                    #I just exported the notebook into python\n",
    "    \n",
    "M = movielens10m.get_movielens_data(local_file=\"./ml-10m.zip\") \n",
    "#I downloaded it with this function and then set local_file not to download several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is it possible to use ```np.linalg.svd``` function to calculate SVD of the downloaded matrices on your laptop? Provide an estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000054\n"
     ]
    }
   ],
   "source": [
    "(train_set,test_set) = movielens10m.split_data(M)\n",
    "print(M.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of double precision representation is 64bit, or 8byte. So in the worst case (it is not, since the matrix is sparse) we need about $8 \\cdot 10^{7}$ byte, which is approximately $80$Mbyte. We have such amount of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Implement function `tr_svd` so that it computes truncated SVD using `scipy.linalg.svds`:\n",
    "    * Be aware that `scipy` returns singular values in ascending order (see the [docs](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.linalg.svds.html)).\n",
    "    * Sort all your svd data in descending (by singular values) order without breaking the result.\n",
    "    \n",
    "\n",
    "* Fix the rank of approximation and compute truncated SVD with `tr_svd` of the training set of the dataset. Plot the obtained singular values. Can you tell from the plot whether the data has a low-rank structure? Give your intuition, why it happens?\n",
    "\n",
    "\n",
    "* Write the function `top_n` which takes user as a row of his/her ratings (including non-rated films, i.e. just a row from the train\\test set), integer number $N$ and returns array of indices which correspond to $N$ highest ratings. Use function `np.argsort()`.\n",
    "\n",
    "\n",
    "* Pick several users at random from the training set. Compare their top-10 films and top-10, suggested by your model ($A_k = U_k \\Sigma_k V_k^T$). Comment on the result. **Note:** you can run all tests in this task with $k=25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55903, 10677)\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# INPUT: A: scipy.sparse.csr_matrix (N_train x N_films), k - integer\n",
    "# OUTPUT: U - np.array (N_train x k), S - np.array (k x k), Vh - np.array (k x N_films)\n",
    "def tr_svd(A, k): # 5 pts\n",
    "    # enter your code here\n",
    "    (U,S,Vh) = sp.sparse.linalg.svds(A,k)\n",
    "    permutationArray = np.argsort(-S)\n",
    "    U=U[:,permutationArray]\n",
    "    Vh=Vh[permutationArray,:]\n",
    "    S=S[permutationArray]\n",
    "    return U, S, Vh\n",
    "\n",
    "# INPUT: user - np.array (N_films,), N - integer \n",
    "# OUTPUT: np.array (N,)\n",
    "def top_n(user, N): # 2 pts\n",
    "    # enter your code here\n",
    "    return np.argsort(-user,axis=0)[:N]\n",
    "\n",
    "print(train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K=25 #approximation rank\n",
    "(U,S,Vt)=tr_svd(train_set,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f000001f98>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdQAAAHUCAYAAACDJ9lsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAIDVZKkEhiokiSVwECVJKkE/x9TVjFipvVzgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f07a19fb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.plot(S)\n",
    "ax.set_title(str(K)+\" largest singular values of training data\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(r\"$\\sigma_k$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55903, 25)\n",
      "(25,)\n",
      "(25, 10677)\n",
      "[40532 12435 22160 29992 21886  4063 49455 40680 21278 32996]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-47bb0790151a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m#print(top_n(Ak[index],10))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "print(U.shape)\n",
    "print(S.shape)\n",
    "print(Vt.shape)\n",
    "#Ak = (U*S).dot(Vt)\n",
    "index=np.random.randint(train_set.shape[0],size=(10,))\n",
    "print(index)\n",
    "#print(top_n(Ak[index],10))\n",
    "print(np.array(train_set[index,:].todense(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singluar values are decreased on approximately one order of magnitude. Intuitively we can claim that this is the evidence of data's low dimensionality. This may be due to existence of whole clusters of users, who prefer watching relatively same movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Evaluating performance of the recommender (15 pts)\n",
    "\n",
    "Suppose, we trained our model (obtain $U_k, \\Sigma_k, V^T_k$ from $A_{train}$). Let's evaluate it! For this purpose we have $A_{test}$ (recall the function [```split_data```](movielens10m.ipynb)). And our goal is to obtain vectors of recommendation $r$ for each user (row) in the test set ($A_{test}$). But there is no need to recompute the whole SVD for each user. We have the tool, which is called _folding-in_ for recommender systems.\n",
    "\n",
    "#### Folding-in technique                                                             \n",
    "\n",
    "<img src=\"decomp.png\" width='450'>\n",
    "\n",
    "\n",
    "A new user can be considered as an update to the original matrix (appending new row). Appending a row in the original matrix corresponds to appending a row into the users latent factors matrix $U_k$ in the SVD decomposition. \n",
    "Since we do not want to recompute the SVD, we project the new user on the space of found latent factors $V_k$, which spans the row space of matrix $A_k = U_k \\Sigma_k V^T_k$ (recall the problem from the theoretical tasks).\n",
    "The orthoprojection on this space is $P = V_kV_k^T$ (check that it satisfies definition of orthoprojection, i.e. $P^2=P$, $P^T = P$).\n",
    "\n",
    "Thus, the recommendation vector $r$ for a new user $x$ (considered as a column vector) can be written as\n",
    "\n",
    "$$\n",
    "r = V_kV_k^T x.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Computing the total score\n",
    "You have to iterate over all users in the test set and make the following steps:\n",
    "* obtain vector $x$, which is the same as user row, but the last $N = 3$ rated films should be filled with zeroes. Example:\n",
    "\n",
    "$$\n",
    "user = (0, 0, 1, 3, 5, 2, 0, 2, 2, 1, 0, 5) \\;\\; \\to \\;\\;  x = (0, 0, 1, 3, 5, 2, 0, 2, 2, 0, 0, 0).\n",
    "$$\n",
    "\n",
    "* compute the folding-in prediction $r$:\n",
    "\n",
    "$$\n",
    "r = V_k V_k^T x.\n",
    "$$\n",
    "\n",
    "* Obtain top-3 from $user$ (truth) and top-3 from $r$ (prediction). The number of films appearing _simultaneously_ in both top-3's  should be added to the `total_score`. Write the corresponding function `total_score_folding`, which takes the sparse test matrix $A_{test}$,  $V_k$ from truncated SVD of $A_{train}$ and compute the total score. \n",
    "\n",
    "**Example: **\n",
    "\n",
    "|    $user$    |     $recommendation$    |\n",
    "|:------------:|:----------:|\n",
    "|    (**1**,**2**,3)   |  (10,**2**,**1**)  |\n",
    "| (34, 27, **69**) | (**69**, 5, 9) |\n",
    "|    (7,6,4)   |   (8,9,2)  |\n",
    "\n",
    "```total_score``` = 2 + 1 + 0 = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT: V - np.array(N_films, k), test_data - scipy.sparse.csr_matrix (N_train x N_films)\n",
    "# OTPUT: total_score - integer\n",
    "def total_score_folding(V, test_data): # 8 pts\n",
    "    # enter you code here\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (bonus) Fine-tuning your model\n",
    "\n",
    "* Try to find the rank that produces the best evaluation score.\n",
    "    * Plot the dependency of evaluation score on the rank of SVD for all your trials in one graph.\n",
    "* Report the best result and the corresponding SVD rank.\n",
    "* Compare your model with the non-personalized recommender which simply recommends top-3 movies with highest average ratings. \n",
    "\n",
    "**Note**, that you don't have to recompute SVD to evaluate your model. You might compute once relatively large ($k =500$) truncated SVD and then just use submatrices of it.\n",
    "\n",
    "**Optionally:**\n",
    "You may want to test your parameters with different data splittings in order to minimize the risk of local effects.\n",
    "You're also free to add modifications to your code for producing better results. Report what modificatons you've done and what effect it had if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 (eigenvalues)\n",
    "\n",
    "## 55 pts\n",
    "\n",
    "### 1. Theoretical tasks (10 pts)\n",
    "\n",
    "* Prove that normal matrix is Hermitian iff its eigenvalues are real. Prove that normal matrix is unitary iff its eigenvalues satisfy $|\\lambda| = 1$. \n",
    "\n",
    "* The following problem illustrates instability of the Jordan form. Find theoretically the eigenvalues of the perturbed Jordan block:\n",
    "\n",
    "$$\n",
    "    J(\\varepsilon) = \n",
    "    \\begin{bmatrix} \n",
    "     \\lambda & 1 & & & 0 \\\\ \n",
    "     & \\lambda & 1 & & \\\\ \n",
    "     &  & \\ddots & \\ddots & \\\\ \n",
    "     & & & \\lambda & 1 \\\\ \n",
    "     \\varepsilon & & & & \\lambda  \\\\ \n",
    "    \\end{bmatrix}_{n\\times n}\n",
    "$$\n",
    "\n",
    "Comment how eigenvalues of $J(0)$ are perturbed for large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PageRank (30 pts)\n",
    "\n",
    "\n",
    "#### Damping factor importance\n",
    "\n",
    "* Write the function ```pagerank_matrix(G)``` that takes an adjacency matrix $G$ as an input and outputs the corresponding PageRank matrix $A$.\n",
    "\n",
    "* Find PageRank matrix $A$ that corresponds to the following graph: <img src=\"graph.png\" width='250'>\n",
    "What is its largest eigenvalue? What multiplicity does it have?\n",
    "\n",
    "\n",
    "* Implement the power method for a given matrix $A$, an initial guess $x_0$ and a number of iterations ```num_iter```. It should be organized as a function ```power_method(A, x0, num_iter)``` that outputs approximation to eigenvector $x$, eigenvalue $\\lambda$ and history of residuals $\\{\\|Ax_k - \\lambda_k x_k\\|_2\\}$. Make sure that the method conveges to the correct solution on a matrix $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$ which is known to have the largest eigenvalue equal to $3$.\n",
    "\n",
    "\n",
    "* Run the power method for the graph presented above and plot residuals $\\|Ax_k - \\lambda_k x_k\\|_2$ as a function of $k$ for ```num_iter=100``` and random initial guess ```x0```.  Explain the absence of convergence. \n",
    "\n",
    "\n",
    "* Consider the same graph, but with the directed edge that goes from the node 3 to the node 4 being removed. Plot residuals as in the previous task and discuss the convergence. Now, run the power method with ```num_iter=100``` for 10 different initial guesses and print/plot the resulting approximated eigenvectors. Why do they depend on the initial guess?\n",
    "\n",
    "\n",
    "In order to avoid this problem Larry Page and Sergey Brin [proposed](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) to use the following regularization technique:\n",
    "\n",
    "$$\n",
    "A_d = dA + \\frac{1-d}{N} \\begin{pmatrix} 1 & \\dots & 1 \\\\ \\vdots & & \\vdots \\\\ 1 & \\dots & 1 \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $d$ is a small parameter in $[0,1]$ (typically $d=0.85$), which is called **damping factor**, $A$ is of size $N\\times N$. Now $A_d$ is the matrix with multiplicity of the largest eigenvalue equal to 1. \n",
    "Recall that computing the eigenvector of the PageRank matrix, which corresponds to the largest eigenvalue, has the following interpretation. Consider a person who stays in a random node of a graph (i.e. opens a random web page); at each step s/he follows one of the outcoming edges uniformly at random (i.e. opens one of the links). So the person randomly walks through the graph and the eigenvector we are looking for is exactly his/her stationary distribution — for each node it tells you the probability of visiting this particular node. Therefore, if the person has started from a part of the graph which is not connected with the other part, he will never get there.  In the regularized model, the person at each step follows one of the outcoming links with probability $d$ OR visits a random node from the whole graph with probability $(1-d)$.\n",
    "\n",
    "* Now, run the power method with $A_d$ and plot residuals $\\|A_d x_k - \\lambda_k x_k\\|_2$ as a function of $k$ for $d=0.99$, ```num_iter=100``` and a random initial guess ```x0```.\n",
    "\n",
    "\n",
    "Usually, graphs that arise in various areas are sparse (social, web, road networks, etc.) and, thus, computation of a matrix-vector product for corresponding PageRank matrix $A$ is much cheaper than $\\mathcal{O}(N^2)$. However, if $A_d$ is calculated directly, it becomes dense and, therefore, $\\mathcal{O}(N^2)$ cost grows prohibitively large for  big $N$.\n",
    "\n",
    "\n",
    "* Implement fast matrix-vector product for $A_d$ as a function ```pagerank_matvec(A, d, x)```, which takes a PageRank matrix $A$ (in sparse format, e.g., ```csr_matrix```), damping factor $d$ and a vector $x$ as an input and returns $A_dx$ as an output. Generate a random adjacency matrix of size $10000 \\times 10000$ with only 100 non-zero elements and compare ```pagerank_matvec``` performance with direct evaluation of $A_dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT:  G - np.ndarray\n",
    "# OUTPUT: A - np.ndarray (of size G.shape)\n",
    "def pagerank_matrix(G): # 5 pts\n",
    "    # enter your code here\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT:  A - np.ndarray (2D), x0 - np.ndarray (1D), num_iter - integer (positive) \n",
    "# OUTPUT: x - np.ndarray (of size x0), l - float, res - np.ndarray (of size num_iter + 1 [include initial guess])\n",
    "def power_method(A, x0, num_iter): # 5 pts\n",
    "    # enter your code here\n",
    "    return x, l, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT:  A - np.ndarray (2D), d - float (from 0.0 to 1.0), x - np.ndarray (1D, size of A.shape[0/1])\n",
    "# OUTPUT: y - np.ndarray (1D, size of x)\n",
    "def pagerank_matvec(A, d, x): # 2 pts\n",
    "    # enter your code here\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBLP: computer science bibliography\n",
    "\n",
    "Download the dataset from [here](https://goo.gl/oZVxEa), unzip it and put `dblp_authors.npz`  and `dblp_graph.npz` in the same folder with this notebook. Each value (author name) from `dblp_authors.npz` corresponds to the row/column of the matrix from `dblp_graph.npz`. Value at row `i` and column `j` of the matrix from `dblp_graph.npz` corresponds to the number of times author `i` cited papers of the author `j`. Let us now find the most significant scientists according to PageRank model over DBLP data.\n",
    "\n",
    "* Load the weighted adjacency matrix and the authors list into Python using ```load_dblp(...)``` function. Print its density (fraction of nonzero elements). Find top-10 most cited authors from the weighted adjacency matrix. Now, make all the weights of the adjacency matrix equal to 1 for simplicity (consider only existence of connection between authors, not its weight). Obtain the PageRank matrix $A$ from the adjacency matrix and verify that it is stochastic.\n",
    " \n",
    " \n",
    "* In order to provide ```pagerank_matvec``` to your ```power_method``` (without rewriting it) for fast calculation of $A_dx$, you can create a ```LinearOperator```: \n",
    "```python\n",
    "L = scipy.sparse.linalg.LinearOperator(A.shape, matvec=lambda x, A=A, d=d: pagerank_matvec(A, d, x))\n",
    "```\n",
    "Calling ```L@x``` or ```L.dot(x)``` will result in calculation of ```pagerank_matvec(A, d, x)``` and, thus, you can plug $L$ instead of the matrix $A$ in the ```power_method``` directly. **Note:** though in the previous subtask graph was very small (so you could disparage fast matvec implementation), here it is very large (but sparse), so that direct evaluation of $A_dx$ will require $\\sim 10^{12}$ matrix elements to store - good luck with that (^_<)≡☆\n",
    "\n",
    "\n",
    "* Run the power method starting from the vector of all ones and plot residuals $\\|A_dx_k - \\lambda_k x_k\\|_2$  as a function of $k$ for $d=0.85$.\n",
    "\n",
    "\n",
    "* Print names of the top-10 authors according to PageRank over DBLP when $d=0.85$.\n",
    "\n",
    "\n",
    "* (Bonus) Does it look suspicious? Why? Discuss what could cause such results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "def load_dblp(path_auth, path_graph):\n",
    "    G = load_npz(path_graph).astype(float)\n",
    "    with np.load(path_auth) as data: authors = data['authors']\n",
    "    return G, authors\n",
    "G, authors = load_dblp('dblp_authors.npz', 'dblp_graph.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QR algorithm (10 pts)\n",
    "\n",
    "* Implement QR-algorithm without shifting. Prototype of the function is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "# A_init - square matrix, \n",
    "# num_iter - number of iterations for QR algorithm\n",
    "# OUTPUT: \n",
    "# Ak - transformed matrix A_init given by QR algorithm, \n",
    "# convergence - numpy array of shape (num_iter, ), \n",
    "# where we store the maximal number from the Chebyshev norm \n",
    "# of triangular part of the Ak for every iteration\n",
    "def qr_algorithm(A_init, num_iter): # 3 pts\n",
    "    # enter your code here\n",
    "    return Ak, convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric case\n",
    "\n",
    "- Create **symmetric** tridiagonal $11 \\times 11$ matrix with elements $-1, 2, -1$ on sub-, main- and upper diagonal respectively without using loops. \n",
    "- Run $300$ iterations of the QR algorithm for this matrix. \n",
    "- Plot the output matrix with function ```plt.spy(Ak, precision=1e-7)```.\n",
    "- Plot convergence of QR-algorithm.\n",
    "\n",
    "<img src=https://pbs.twimg.com/media/Bq6t17OIMAALkiA.jpg width=30%/>\n",
    "*Photo comment*: professor Gilbert Strang (MIT) \"These are 121 cupcakes with my favorite -1, 2, -1 matrix. It was the day before Thanksgiving and two days before my birthday. A happy surprise.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonsymmetric case\n",
    "\n",
    "- Create **nonsymmetric** tridiagonal $11 \\times 11$ matrix with elements $5, 3, -2$ on sub-, main- and upper diagonal respectively without using loops. \n",
    "- Run $200$ iterations of the QR algorithm for this matrix. \n",
    "- Plot the result matrix with function ```plt.spy(Ak, precision=1e-7)```. Is this matrix lower triangular? How does this correspond to the claim about convergence of the QR algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
